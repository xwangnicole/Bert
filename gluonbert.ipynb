{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:01:08.031699Z",
     "start_time": "2019-08-12T15:01:04.031413Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from sentence_embedding.bert import data, model\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:01:12.017268Z",
     "start_time": "2019-08-12T15:01:09.204579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:04:51.784874Z",
     "start_time": "2019-08-12T15:04:51.722361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough alliant credit unions brick and mortar\n",
      "3\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2025  2438  2035  2937  2102  4923  9209  5318  1998 14335     3\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "12\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# read in train data set\n",
    "\n",
    "def read_in_tokenizer(filename):\n",
    "    num_discard_samples = 1\n",
    "    # by comma\n",
    "    field_separator = nlp.data.Splitter(',')\n",
    "    # Fields to select from the file\n",
    "    field_indices = [0,1] #[3,0]\n",
    "    data_train_raw = nlp.data.TSVDataset(filename,\n",
    "                                     field_separator=field_separator,\n",
    "                                     num_discard_samples=num_discard_samples,\n",
    "                                     field_indices=field_indices)\n",
    "    # Sentence A & target\n",
    "    sample_id=0\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    \n",
    "    # Use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "    # The maximum length of an input sequence\n",
    "    max_len = 128\n",
    "\n",
    "    # The labels for the 4 classes\n",
    "    all_labels = [\"0\", \"1\",\"2\",\"3\"]\n",
    "    transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                    class_labels=all_labels,\n",
    "                                                    has_label=True,\n",
    "                                                    pad=True,\n",
    "                                                    pair=False)\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "    print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "    print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "\n",
    "    return data_train_raw.transform(transform)\n",
    "    \n",
    "# trainfilename =\"C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/train.tsv\"\n",
    "trainfilename ='C:/Users/nwang/Desktop/nlp/code/q1_balance_1152_5_train.csv'\n",
    "# \"C:/Users/nwang/Desktop/nlp/code/q2_balance_339_5_train.csv\"\n",
    "data_train = read_in_tokenizer(trainfilename)\n",
    "sample_id=0\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:01:36.870920Z",
     "start_time": "2019-08-12T15:01:36.839658Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.classification.BERTClassifier(bert_base, num_classes=4, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:33:57.506238Z",
     "start_time": "2019-08-12T15:33:57.475008Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_train(data_train):\n",
    "    # The hyperparameters\n",
    "    batch_size =32\n",
    "    lr = 5e-6 #change from 6\n",
    "\n",
    "    # The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "    train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "    trainer = mx.gluon.Trainer(model.collect_params(), 'adam',\n",
    "                               {'learning_rate': lr, 'epsilon': 1e-9, 'wd': 0.01},update_on_kvstore=False)\n",
    "\n",
    "    # Collect all differentiable parameters\n",
    "    # `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "    # The gradients for these params are clipped later\n",
    "    params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "    \n",
    "    log_interval =8\n",
    "    num_epochs = 10 #change from 3\n",
    "    for epoch_id in range(num_epochs):\n",
    "        metric.reset()\n",
    "        step_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "            with mx.autograd.record():\n",
    "\n",
    "                # Load the data\n",
    "                token_ids = token_ids.as_in_context(ctx)\n",
    "                valid_length = valid_length.as_in_context(ctx)\n",
    "                segment_ids = segment_ids.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "\n",
    "                # Forward computation\n",
    "                out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "                ls = loss_function(out, label).mean()\n",
    "\n",
    "            # And backwards computation\n",
    "            ls.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            trainer.allreduce_grads()\n",
    "            nlp.utils.clip_grad_global_norm(params, 1)\n",
    "            trainer.update(1)\n",
    "\n",
    "            step_loss += ls.asscalar()\n",
    "            metric.update([label], [out])\n",
    "\n",
    "            # Printing vital information\n",
    "            if (batch_id + 1) % (log_interval) == 0:\n",
    "                print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                             .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                     step_loss / log_interval,\n",
    "                                     trainer.learning_rate, metric.get()[1]))\n",
    "                step_loss = 0\n",
    "        mx.nd.waitall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T19:23:44.365156Z",
     "start_time": "2019-08-12T15:34:48.753581Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 8/40] loss=0.1221, lr=0.0000050, acc=0.983\n",
      "[Epoch 0 Batch 16/40] loss=0.1633, lr=0.0000050, acc=0.980\n",
      "[Epoch 0 Batch 24/40] loss=0.1407, lr=0.0000050, acc=0.975\n",
      "[Epoch 0 Batch 32/40] loss=0.1654, lr=0.0000050, acc=0.970\n",
      "[Epoch 0 Batch 40/40] loss=0.1706, lr=0.0000050, acc=0.967\n",
      "[Epoch 1 Batch 8/40] loss=0.1528, lr=0.0000050, acc=0.965\n",
      "[Epoch 1 Batch 16/40] loss=0.1178, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 24/40] loss=0.1952, lr=0.0000050, acc=0.962\n",
      "[Epoch 1 Batch 32/40] loss=0.1420, lr=0.0000050, acc=0.963\n",
      "[Epoch 1 Batch 40/40] loss=0.1422, lr=0.0000050, acc=0.962\n",
      "[Epoch 2 Batch 8/40] loss=0.1321, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 16/40] loss=0.1653, lr=0.0000050, acc=0.963\n",
      "[Epoch 2 Batch 24/40] loss=0.1110, lr=0.0000050, acc=0.965\n",
      "[Epoch 2 Batch 32/40] loss=0.1553, lr=0.0000050, acc=0.965\n",
      "[Epoch 2 Batch 40/40] loss=0.1621, lr=0.0000050, acc=0.964\n",
      "[Epoch 3 Batch 8/40] loss=0.1002, lr=0.0000050, acc=0.964\n",
      "[Epoch 3 Batch 16/40] loss=0.1172, lr=0.0000050, acc=0.975\n",
      "[Epoch 3 Batch 24/40] loss=0.1272, lr=0.0000050, acc=0.975\n",
      "[Epoch 3 Batch 32/40] loss=0.1942, lr=0.0000050, acc=0.967\n",
      "[Epoch 3 Batch 40/40] loss=0.1332, lr=0.0000050, acc=0.969\n",
      "[Epoch 4 Batch 8/40] loss=0.1093, lr=0.0000050, acc=0.979\n",
      "[Epoch 4 Batch 16/40] loss=0.1167, lr=0.0000050, acc=0.979\n",
      "[Epoch 4 Batch 24/40] loss=0.1080, lr=0.0000050, acc=0.979\n",
      "[Epoch 4 Batch 32/40] loss=0.1971, lr=0.0000050, acc=0.975\n",
      "[Epoch 4 Batch 40/40] loss=0.1806, lr=0.0000050, acc=0.966\n",
      "[Epoch 5 Batch 8/40] loss=0.1184, lr=0.0000050, acc=0.973\n",
      "[Epoch 5 Batch 16/40] loss=0.1496, lr=0.0000050, acc=0.961\n",
      "[Epoch 5 Batch 24/40] loss=0.1317, lr=0.0000050, acc=0.965\n",
      "[Epoch 5 Batch 32/40] loss=0.1535, lr=0.0000050, acc=0.964\n",
      "[Epoch 5 Batch 40/40] loss=0.1256, lr=0.0000050, acc=0.966\n",
      "[Epoch 6 Batch 8/40] loss=0.1141, lr=0.0000050, acc=0.967\n",
      "[Epoch 6 Batch 16/40] loss=0.1467, lr=0.0000050, acc=0.958\n",
      "[Epoch 6 Batch 24/40] loss=0.1170, lr=0.0000050, acc=0.965\n",
      "[Epoch 6 Batch 32/40] loss=0.1518, lr=0.0000050, acc=0.963\n",
      "[Epoch 6 Batch 40/40] loss=0.1342, lr=0.0000050, acc=0.964\n",
      "[Epoch 7 Batch 8/40] loss=0.1200, lr=0.0000050, acc=0.975\n",
      "[Epoch 7 Batch 16/40] loss=0.1266, lr=0.0000050, acc=0.967\n",
      "[Epoch 7 Batch 24/40] loss=0.1757, lr=0.0000050, acc=0.965\n",
      "[Epoch 7 Batch 32/40] loss=0.1261, lr=0.0000050, acc=0.965\n",
      "[Epoch 7 Batch 40/40] loss=0.1072, lr=0.0000050, acc=0.969\n",
      "[Epoch 8 Batch 8/40] loss=0.1208, lr=0.0000050, acc=0.967\n",
      "[Epoch 8 Batch 16/40] loss=0.1002, lr=0.0000050, acc=0.976\n",
      "[Epoch 8 Batch 24/40] loss=0.1320, lr=0.0000050, acc=0.972\n",
      "[Epoch 8 Batch 32/40] loss=0.1517, lr=0.0000050, acc=0.969\n",
      "[Epoch 8 Batch 40/40] loss=0.1123, lr=0.0000050, acc=0.969\n",
      "[Epoch 9 Batch 8/40] loss=0.1043, lr=0.0000050, acc=0.976\n",
      "[Epoch 9 Batch 16/40] loss=0.1177, lr=0.0000050, acc=0.979\n",
      "[Epoch 9 Batch 24/40] loss=0.1553, lr=0.0000050, acc=0.975\n",
      "[Epoch 9 Batch 32/40] loss=0.1094, lr=0.0000050, acc=0.975\n",
      "[Epoch 9 Batch 40/40] loss=0.1310, lr=0.0000050, acc=0.973\n"
     ]
    }
   ],
   "source": [
    "bert_train(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T21:51:40.834473Z",
     "start_time": "2019-08-02T21:51:40.803214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('accuracy', 0.3333333333333333)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = mx.nd.array([[ 5.64021111e-01,  6.42217535e-01 , 3.33708376e-02 , 3.21580499e-01],\n",
    " [ 5.36897779e-01, -6.86344206e-02 , 1.65089130e-01 , 5.23052096e-01],\n",
    " [ 3.70361090e-01, -1.75818592e-01 , 3.87940586e-01,  4.53780949e-01]])\n",
    "labels   = mx.nd.array([[1],\n",
    " [3],\n",
    " [2]])\n",
    "acc = mx.metric.Accuracy()\n",
    "acc.update(preds = predicts, labels = labels)\n",
    "acc.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:05:32.060730Z",
     "start_time": "2019-08-12T15:05:32.029505Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data_train):\n",
    "    batch_size = 32\n",
    "\n",
    "    train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "    metric.reset()\n",
    "    results = []\n",
    "    for _, seqs in enumerate(bert_dataloader):\n",
    "        input_ids, valid_len, type_ids, label = seqs\n",
    "        out = model(\n",
    "            input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n",
    "            valid_len.astype('float32').as_in_context(ctx))\n",
    "\n",
    "        metric.update([label], [out])\n",
    "        indices = mx.nd.topk(out, k=1, ret_typ='indices', dtype='int32').asnumpy()\n",
    "        for index in indices:\n",
    "            results.append(int(index))\n",
    "#     print(results)\n",
    "    metric_nm, metric_val = metric.get()\n",
    "    if not isinstance(metric_nm, list):\n",
    "        metric_nm, metric_val = [metric_nm], [metric_val]\n",
    "    metric_str = 'validation metrics:' + ','.join([i + ':%.4f' for i in metric_nm])\n",
    "\n",
    "    return metric_nm, metric_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T19:47:38.635108Z",
     "start_time": "2019-08-12T19:47:38.338223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now that weâ€™ve resolved the issue with my credit card I am extremely pleased\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n"
     ]
    }
   ],
   "source": [
    "testfilename = \"C:/Users/nwang/Desktop/nlp/code/q1_balance_384_5_test.csv\"\n",
    "data_test = read_in_tokenizer(testfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T20:32:06.304784Z",
     "start_time": "2019-08-12T19:47:50.990922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['accuracy'], [0.796875])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T03:03:23.186805Z",
     "start_time": "2019-08-05T03:02:39.512048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 2, 2, 2, 3, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2, 2, 3, 1, 2, 1, 0, 1, 1, 0, 1, 3, 1, 2, 3, 0, 0, 3, 1, 1, 0, 0, 0, 2, 2, 2, 3, 2, 1, 1, 1, 2, 1, 2, 3, 2, 3, 2, 2, 0, 1, 2, 3, 1, 2, 3, 2, 2, 1, 1, 1, 1, 3, 2, 2, 0, 0, 3, 0, 1, 3, 3, 0, 0, 0, 0, 1, 1, 0, 0, 3, 0, 1, 0, 3, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 3, 1, 3, 3, 0, 3, 1, 3, 3, 0, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['accuracy'], [0.831858407079646])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T19:42:25.734443Z",
     "start_time": "2019-08-12T19:42:18.937294Z"
    }
   },
   "outputs": [],
   "source": [
    "params_saved='C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/q1_model18_70'\n",
    "nlp.utils.save_parameters(model, params_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T15:01:45.811113Z",
     "start_time": "2019-08-12T15:01:45.014197Z"
    }
   },
   "outputs": [],
   "source": [
    "params_saved='C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/q1_model18_70'\n",
    "nlp.utils.load_parameters(model, params_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T01:40:08.279427Z",
     "start_time": "2019-08-04T01:39:26.454796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None it was perfect Very easy\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(filename):\n",
    "    \n",
    "    num_discard_samples = 0\n",
    "    # by comma\n",
    "    field_separator = nlp.data.Splitter('\\t')\n",
    "    # Fields to select from the file\n",
    "    field_indices = [0] #[3,0]\n",
    "    data_train_raw = nlp.data.TSVDataset(filename,\n",
    "                                     field_separator=field_separator,\n",
    "                                     num_discard_samples=num_discard_samples,\n",
    "                                     field_indices=field_indices)\n",
    "    # Sentence A & target\n",
    "    sample_id=0\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    \n",
    "    # Use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "    # The maximum length of an input sequence\n",
    "    max_len = 128\n",
    "\n",
    "    # The labels for the 4 classes\n",
    "#     all_labels = [\"0\", \"1\"]\n",
    "    transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "#                                                     class_labels=all_labels,\n",
    "                                                    has_label=False,\n",
    "                                                    pad=True,\n",
    "                                                    pair=False)\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "    print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "    print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "    metric.reset()       \n",
    "        \n",
    "    results = []\n",
    "    for _, seqs in enumerate(bert_dataloader):\n",
    "        input_ids, valid_length, type_ids = seqs\n",
    "        out = model(input_ids.as_in_context(ctx),\n",
    "                    type_ids.as_in_context(ctx),\n",
    "                    valid_length.astype('float32').as_in_context(ctx))\n",
    "        \n",
    "        indices = mx.nd.topk(out, k=1, ret_typ='indices', dtype='int32').asnumpy()\n",
    "\n",
    "        for index in indices:\n",
    "            results.append(int(index))\n",
    "\n",
    "    mx.nd.waitall()\n",
    "    return results\n",
    "\n",
    "testfilename =\"C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/dev.tsv\"\n",
    "test(testfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
