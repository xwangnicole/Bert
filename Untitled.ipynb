{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T04:52:13.394054Z",
     "start_time": "2019-08-04T04:52:13.362796Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from sentence_embedding.bert import data, model\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(testfilename,params_saved ):\n",
    "\n",
    "    from sentence_embedding.bert import model\n",
    "    bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                                 dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                                 use_decoder=False, use_classifier=False)\n",
    "\n",
    "    model = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    \n",
    "    nlp.utils.load_parameters(model, params_saved)\n",
    "    \n",
    "    #test\n",
    "    num_discard_samples = 0\n",
    "    # by comma\n",
    "    field_separator = nlp.data.Splitter('\\t')\n",
    "    # Fields to select from the file\n",
    "    field_indices = [0] #[3,0]\n",
    "    data_train_raw = nlp.data.TSVDataset(testfilename,\n",
    "                                     field_separator=field_separator,\n",
    "                                     num_discard_samples=num_discard_samples,\n",
    "                                     field_indices=field_indices)\n",
    "    # Sentence A & target\n",
    "    sample_id=0\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    \n",
    "    # Use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "    # The maximum length of an input sequence\n",
    "    max_len = 128\n",
    "\n",
    "    # The labels for the 4 classes\n",
    "#     all_labels = [\"0\", \"1\"]\n",
    "    transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "#                                                     class_labels=all_labels,\n",
    "                                                    has_label=False,\n",
    "                                                    pad=True,\n",
    "                                                    pair=False)\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "    print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "    print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "    metric.reset()       \n",
    "        \n",
    "    results = []\n",
    "    for _, seqs in enumerate(bert_dataloader):\n",
    "        input_ids, valid_length, type_ids = seqs\n",
    "        out = model(input_ids.as_in_context(ctx),\n",
    "                    type_ids.as_in_context(ctx),\n",
    "                    valid_length.astype('float32').as_in_context(ctx))\n",
    "        \n",
    "        indices = mx.nd.topk(out, k=1, ret_typ='indices', dtype='int32').asnumpy()\n",
    "        for index in indices:\n",
    "            results.append(int(index))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "testfilename =\"C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/dev.tsv\"\n",
    "params_saved=['C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/model_1',\n",
    "              'C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/model_2',\n",
    "              'C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/model_3',\n",
    "              'C:/Users/nwang/Desktop/nlp/bert/data/q2_balance_113_5/4classes/model_4']\n",
    "\n",
    "c=0\n",
    "for pa in params_saved:\n",
    "    if c==0:\n",
    "        one=model4(testfilename,pa)\n",
    "        one=np.array(one).reshape((len(one),1))\n",
    "        c=2\n",
    "    else:\n",
    "        tmp=model4(testfilename,pa)\n",
    "        tmp=np.array(tmp).reshape((len(one),1))\n",
    "        one=np.hstack((one,tmp))\n",
    "        \n",
    "one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
